{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup \n",
    "import random\n",
    "import time\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from util import *\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_url_created (keyword,since,until):\n",
    "    keykata = keyword.split(' ')\n",
    "    keydate = \"%20until%3A{}%20since%3A{}\".format(until,since)\n",
    "    if len(keykata) >=2:\n",
    "        keysearch = '%20'.join(keykata)\n",
    "        link = \"https://twitter.com/search?lang=id&q={}{}&src=typed_query\".format(keysearch,keydate)\n",
    "    else:\n",
    "        link = \"https://twitter.com/search?lang=id&q={}{}&src=typed_query\".format(keyword,keydate)\n",
    "    \n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = Options()\n",
    "driver = webdriver.Chrome(options=option, executable_path=r'../chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/search?lang=id&q=covid19%20until%3A2020-5-9%20since%3A2020-5-2&src=typed_query\n"
     ]
    }
   ],
   "source": [
    "keyword = \"covid19\"\n",
    "since = \"2020-5-2\"\n",
    "until = \"2020-5-9\"\n",
    "print(base_url_created(keyword, since, until))\n",
    "driver.get(base_url_created(keyword, since, until))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scroll Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = 5\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "for j in range(pages):\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "    rand = random.randint(1,5)\n",
    "    time.sleep(rand)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "post=soup.find_all(\"div\",{\"class\":\"css-1dbjc4n\",\"data-testid\":\"tweet\"},recursive=True)\n",
    "len(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Detail From Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Detail From 1 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 2 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 3 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 4 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 5 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 6 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 7 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 8 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 9 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 10 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 11 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 12 Post\n",
      "Saved Into Excel File\n",
      "Get Detail From 13 Post\n",
      "Saved Into Excel File\n"
     ]
    }
   ],
   "source": [
    "for i in (range(len(post))):\n",
    "    time.sleep(4)\n",
    "    print(\"Get Detail From {} Post\".format(str(i+1)))\n",
    "    df0 = pandas.DataFrame({\"post\":[],\n",
    "                            \"comment\": [],\n",
    "                            \"parrent_comment\":[]})\n",
    "    \n",
    "    caption, url_post = get_post(post[i])\n",
    "    \n",
    "    driver.get(url_post)\n",
    "    pages = 10\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for j in range(pages):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        rand = random.randint(5,10)\n",
    "        time.sleep(rand)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    df0 = df0.append(get_comment(soup, caption, df0))\n",
    "    \n",
    "    df0.to_excel(\"data/covid19/{}-{}.xlsx\".format(keyword, i))\n",
    "    print(\"Saved Into Excel File\")\n",
    "    \n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.DataFrame()\n",
    "for file in glob(\"data/covid19/*.xlsx\"):\n",
    "    new_data = pandas.read_excel(file)\n",
    "    data = data.append(new_data)\n",
    "data.to_excel(\"../../Emotion-Indo/covid/merged-covid19-twitter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
